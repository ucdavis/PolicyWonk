This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: experiments/, .env
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
alembic/
  versions/
    40e8eb8de8b2_docs_identity.py
    b83f8109ce65_initial_create.py
    d5b6aaeb712e_docs_should_be_identity.py
    d6082e2cc4a3_docs_should_be_int_id.py
    f87f07e6f643_create_embedding_tables.py
  env.py
  README
  script.py.mako
background/
  sources/
    ingestion.py
    shared.py
    ucop.py
  logger.py
  stream.py
  update.py
db/
  connection.py
  constants.py
  models.py
  mutations.py
  queries.py
dev/
  reset_db.py
  reset_to_dev_state.py
models/
  document_details.py
alembic.ini
env.py
README.md
requirements.txt

================================================================
Files
================================================================

================
File: alembic/versions/40e8eb8de8b2_docs_identity.py
================
"""docs identity

Revision ID: 40e8eb8de8b2
Revises: d5b6aaeb712e
Create Date: 2025-03-06 21:43:41.799788

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '40e8eb8de8b2'
down_revision: Union[str, None] = 'd5b6aaeb712e'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Add IDENTITY (autoincrement) to documents.id column
    op.execute(
        'ALTER TABLE documents ALTER COLUMN id ADD GENERATED BY DEFAULT AS IDENTITY')


def downgrade() -> None:
    # Remove IDENTITY from documents.id column
    op.execute('ALTER TABLE documents ALTER COLUMN id DROP IDENTITY IF EXISTS')

================
File: alembic/versions/b83f8109ce65_initial_create.py
================
"""initial create

Revision ID: b83f8109ce65
Revises: 
Create Date: 2025-03-03 23:44:10.432049

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'b83f8109ce65'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('assistants',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('slug', sa.String(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=False),
    sa.Column('theme', sa.String(), nullable=False),
    sa.Column('instructions', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('slug')
    )
    op.create_table('collections',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('created_date', sa.DateTime(), nullable=False),
    sa.Column('active', sa.Boolean(), nullable=False),
    sa.Column('requires_sync', sa.Boolean(), nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('name')
    )
    op.create_table('prompts',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('prompt', sa.String(), nullable=False),
    sa.Column('expected_output', sa.String(), nullable=True),
    sa.Column('category', sa.String(), nullable=False),
    sa.Column('comments', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('roles',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('name', sa.Enum('ADMIN', 'USER', name='rolename', native_enum=False), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('sources',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('url', sa.String(), nullable=False),
    sa.Column('refresh_frequency', sa.Enum('DAILY', 'WEEKLY', name='refreshfrequency', native_enum=False), nullable=False),
    sa.Column('status', sa.Enum('ACTIVE', 'INACTIVE', 'FAILED', 'DEACTIVATE', name='sourcestatus', native_enum=False), nullable=False),
    sa.Column('type', sa.Enum('UCOP', 'UCDPOLICYMANUAL', 'UCCONTRACTS', 'RECURSIVE', 'SITEMAP', name='sourcetype', native_enum=False), nullable=False),
    sa.Column('config', sa.JSON(), nullable=True),
    sa.Column('last_updated', sa.DateTime(), nullable=True),
    sa.Column('last_failed', sa.DateTime(), nullable=True),
    sa.Column('failure_count', sa.Integer(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_source_refresh_status_last_updated', 'sources', ['refresh_frequency', 'status', 'last_updated'], unique=False)
    op.create_table('users',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('email', sa.String(), nullable=False),
    sa.Column('kerberos', sa.String(length=20), nullable=True),
    sa.Column('iam', sa.String(length=10), nullable=True),
    sa.Column('ms_user_id', sa.String(), nullable=False),
    sa.Column('titles', sa.String(), nullable=True),
    sa.Column('affiliations', sa.String(), nullable=True),
    sa.Column('departments', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('ms_user_id')
    )
    op.create_index('ix_users_email', 'users', ['email'], unique=False)
    op.create_index('ix_users_kerberos', 'users', ['kerberos'], unique=False)
    op.create_index('ix_users_name', 'users', ['name'], unique=False)
    op.create_table('assistant_collections',
    sa.Column('assistant_id', sa.Integer(), nullable=False),
    sa.Column('collection_id', sa.Integer(), nullable=False),
    sa.ForeignKeyConstraint(['assistant_id'], ['assistants.id'], ),
    sa.ForeignKeyConstraint(['collection_id'], ['collections.id'], ),
    sa.PrimaryKeyConstraint('assistant_id', 'collection_id')
    )
    op.create_index('ix_assistant_collections_assistant_id', 'assistant_collections', ['assistant_id'], unique=False)
    op.create_index('ix_assistant_collections_collection_id', 'assistant_collections', ['collection_id'], unique=False)
    op.create_table('chats',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('title', sa.String(), nullable=False),
    sa.Column('messages', sa.JSON(), nullable=False),
    sa.Column('assistant_slug', sa.String(), nullable=False),
    sa.Column('llm_model', sa.String(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.Column('share_id', sa.String(), nullable=True),
    sa.Column('active', sa.Boolean(), nullable=False),
    sa.Column('meta', sa.JSON(), nullable=True),
    sa.ForeignKeyConstraint(['assistant_slug'], ['assistants.slug'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_id_active', 'chats', ['id', 'active'], unique=False)
    op.create_index('idx_userid_active_timestamp', 'chats', ['user_id', 'active', 'timestamp'], unique=False)
    op.create_table('collections_sources',
    sa.Column('collection_id', sa.Integer(), nullable=False),
    sa.Column('source_id', sa.Integer(), nullable=False),
    sa.ForeignKeyConstraint(['collection_id'], ['collections.id'], ),
    sa.ForeignKeyConstraint(['source_id'], ['sources.id'], ),
    sa.PrimaryKeyConstraint('collection_id', 'source_id')
    )
    op.create_index('ix_collections_sources_collection_id', 'collections_sources', ['collection_id'], unique=False)
    op.create_index('ix_collections_sources_source_id', 'collections_sources', ['source_id'], unique=False)
    op.create_table('default_questions',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('assistant_slug', sa.String(), nullable=False),
    sa.Column('question', sa.String(), nullable=False),
    sa.ForeignKeyConstraint(['assistant_slug'], ['assistants.slug'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('documents',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('title', sa.String(), nullable=False),
    sa.Column('url', sa.String(), nullable=True),
    sa.Column('meta', sa.JSON(), nullable=True),
    sa.Column('source_id', sa.Integer(), nullable=False),
    sa.Column('last_updated', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['source_id'], ['sources.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_documents_url'), 'documents', ['url'], unique=False)
    op.create_table('evaluations',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('assistant_id', sa.Integer(), nullable=False),
    sa.Column('pipeline_version', sa.String(), nullable=False),
    sa.Column('comments', sa.String(), nullable=True),
    sa.Column('run_date', sa.DateTime(), nullable=False),
    sa.Column('overall_score', sa.Float(), nullable=False),
    sa.ForeignKeyConstraint(['assistant_id'], ['assistants.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('index_attempts',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('status', sa.Enum('SUCCESS', 'FAILURE', 'INPROGRESS', name='indexstatus', native_enum=False), nullable=False),
    sa.Column('num_docs_indexed', sa.Integer(), nullable=False),
    sa.Column('num_new_docs', sa.Integer(), nullable=False),
    sa.Column('num_docs_removed', sa.Integer(), nullable=False),
    sa.Column('start_time', sa.DateTime(), nullable=False),
    sa.Column('end_time', sa.DateTime(), nullable=True),
    sa.Column('duration', sa.Integer(), nullable=True),
    sa.Column('error_details', sa.String(), nullable=True),
    sa.Column('source_id', sa.Integer(), nullable=False),
    sa.ForeignKeyConstraint(['source_id'], ['sources.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('user_roles',
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('role_id', sa.Integer(), nullable=False),
    sa.ForeignKeyConstraint(['role_id'], ['roles.id'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('user_id', 'role_id')
    )
    op.create_table('prompt_evaluations',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('prompt_id', sa.Integer(), nullable=False),
    sa.Column('evaluation_id', sa.Integer(), nullable=False),
    sa.Column('actual_output', sa.String(), nullable=False),
    sa.Column('context', sa.String(), nullable=True),
    sa.Column('scores', sa.JSON(), nullable=True),
    sa.Column('overall_score', sa.Float(), nullable=False),
    sa.ForeignKeyConstraint(['evaluation_id'], ['evaluations.id'], ),
    sa.ForeignKeyConstraint(['prompt_id'], ['prompts.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('prompt_evaluations')
    op.drop_table('user_roles')
    op.drop_table('index_attempts')
    op.drop_table('evaluations')
    op.drop_index(op.f('ix_documents_url'), table_name='documents')
    op.drop_table('documents')
    op.drop_table('default_questions')
    op.drop_index('ix_collections_sources_source_id', table_name='collections_sources')
    op.drop_index('ix_collections_sources_collection_id', table_name='collections_sources')
    op.drop_table('collections_sources')
    op.drop_index('idx_userid_active_timestamp', table_name='chats')
    op.drop_index('idx_id_active', table_name='chats')
    op.drop_table('chats')
    op.drop_index('ix_assistant_collections_collection_id', table_name='assistant_collections')
    op.drop_index('ix_assistant_collections_assistant_id', table_name='assistant_collections')
    op.drop_table('assistant_collections')
    op.drop_index('ix_users_name', table_name='users')
    op.drop_index('ix_users_kerberos', table_name='users')
    op.drop_index('ix_users_email', table_name='users')
    op.drop_table('users')
    op.drop_index('ix_source_refresh_status_last_updated', table_name='sources')
    op.drop_table('sources')
    op.drop_table('roles')
    op.drop_table('prompts')
    op.drop_table('collections')
    op.drop_table('assistants')
    # ### end Alembic commands ###

================
File: alembic/versions/d5b6aaeb712e_docs_should_be_identity.py
================
"""docs should be identity

Revision ID: d5b6aaeb712e
Revises: d6082e2cc4a3
Create Date: 2025-03-06 21:42:40.540987

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'd5b6aaeb712e'
down_revision: Union[str, None] = 'd6082e2cc4a3'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint('document_contents_document_id_fkey',
                       'document_contents', type_='foreignkey')
    op.create_foreign_key('document_contents_document_id_fkey', 'document_contents', 'documents', [
                          'document_id'], ['id'], ondelete='CASCADE')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint('document_contents_document_id_fkey',
                       'document_contents', type_='foreignkey')
    op.create_foreign_key('document_contents_document_id_fkey',
                          'document_contents', 'documents', ['document_id'], ['id'])
    # ### end Alembic commands ###

================
File: alembic/versions/d6082e2cc4a3_docs_should_be_int_id.py
================
"""docs should be int id

Revision ID: d6082e2cc4a3
Revises: f87f07e6f643
Create Date: 2025-03-06 21:31:20.838799

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'd6082e2cc4a3'
down_revision: Union[str, None] = 'f87f07e6f643'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None

# here we convert doc_id from str -> int
# before we were using url as PK but it seems to me multiple sources could index the same url


def upgrade() -> None:
    # First drop the foreign key constraints
    op.execute(
        'ALTER TABLE document_chunks DROP CONSTRAINT IF EXISTS document_chunks_document_id_fkey')
    op.execute(
        'ALTER TABLE document_contents DROP CONSTRAINT IF EXISTS document_contents_document_id_fkey')

    # Now convert all columns to INTEGER
    op.execute(
        'ALTER TABLE documents ALTER COLUMN id TYPE INTEGER USING id::INTEGER')
    op.execute(
        'ALTER TABLE document_contents ALTER COLUMN document_id TYPE INTEGER USING document_id::INTEGER')
    op.execute(
        'ALTER TABLE document_chunks ALTER COLUMN document_id TYPE INTEGER USING document_id::INTEGER')

    # Re-create the foreign key constraints
    op.execute('ALTER TABLE document_chunks ADD CONSTRAINT document_chunks_document_id_fkey FOREIGN KEY (document_id) REFERENCES documents (id)')
    op.execute('ALTER TABLE document_contents ADD CONSTRAINT document_contents_document_id_fkey FOREIGN KEY (document_id) REFERENCES documents (id)')


def downgrade() -> None:
    # First drop the foreign key constraints
    op.execute(
        'ALTER TABLE document_chunks DROP CONSTRAINT IF EXISTS document_chunks_document_id_fkey')
    op.execute(
        'ALTER TABLE document_contents DROP CONSTRAINT IF EXISTS document_contents_document_id_fkey')

    # Now convert all columns back to VARCHAR
    op.execute(
        'ALTER TABLE documents ALTER COLUMN id TYPE VARCHAR USING id::VARCHAR')
    op.execute(
        'ALTER TABLE document_contents ALTER COLUMN document_id TYPE VARCHAR USING document_id::VARCHAR')
    op.execute(
        'ALTER TABLE document_chunks ALTER COLUMN document_id TYPE VARCHAR USING document_id::VARCHAR')

    # Re-create the foreign key constraints
    op.execute('ALTER TABLE document_chunks ADD CONSTRAINT document_chunks_document_id_fkey FOREIGN KEY (document_id) REFERENCES documents (id)')
    op.execute('ALTER TABLE document_contents ADD CONSTRAINT document_contents_document_id_fkey FOREIGN KEY (document_id) REFERENCES documents (id)')

================
File: alembic/versions/f87f07e6f643_create_embedding_tables.py
================
"""create embedding tables

Revision ID: f87f07e6f643
Revises: b83f8109ce65
Create Date: 2025-03-05 19:37:51.755864

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
import pgvector

# revision identifiers, used by Alembic.
revision: str = 'f87f07e6f643'
down_revision: Union[str, None] = 'b83f8109ce65'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # enable the vector extension
    op.execute('CREATE EXTENSION IF NOT EXISTS vector;')

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('document_chunks',
                    sa.Column('id', sa.Integer(),
                              autoincrement=True, nullable=False),
                    sa.Column('document_id', sa.String(), nullable=False),
                    sa.Column('chunk_index', sa.Integer(), nullable=False),
                    sa.Column('chunk_text', sa.Text(), nullable=False),
                    sa.Column('embedding', pgvector.sqlalchemy.vector.VECTOR(
                        dim=1536), nullable=False),
                    sa.Column('meta', sa.JSON(), nullable=True),
                    sa.ForeignKeyConstraint(
                        ['document_id'], ['documents.id'], ),
                    sa.PrimaryKeyConstraint('id')
                    )
    op.create_index('document_chunks_embedding_idx', 'document_chunks', [
                    'embedding'], unique=False, postgresql_using='hnsw', postgresql_ops={'embedding': 'vector_cosine_ops'})
    op.create_table('document_contents',
                    sa.Column('document_id', sa.String(), nullable=False),
                    sa.Column('content', sa.Text(), nullable=False),
                    sa.ForeignKeyConstraint(
                        ['document_id'], ['documents.id'], ondelete='CASCADE'),
                    sa.PrimaryKeyConstraint('document_id')
                    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('document_contents')
    op.drop_index('document_chunks_embedding_idx', table_name='document_chunks',
                  postgresql_using='hnsw', postgresql_ops={'embedding': 'vector_cosine_ops'})
    op.drop_table('document_chunks')
    # ### end Alembic commands ###
    # disable the vector extension
    op.execute('DROP EXTENSION IF EXISTS vector;')

================
File: alembic/env.py
================
import os
from dotenv import load_dotenv
from db.models import Base
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

load_dotenv()

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# override the sqlalchemy.url value from the config file with our env
database_url = os.getenv(
    'DATABASE_URL', '')
config.set_main_option('sqlalchemy.url', database_url)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata


# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

================
File: alembic/README
================
Generic single-database configuration.

## Create a new migration

```bash
alembic revision --autogenerate -m "create table"
```

## Run migrations

```bash
alembic upgrade head
```

================
File: alembic/script.py.mako
================
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}

================
File: background/sources/ingestion.py
================
from docling.document_converter import DocumentConverter


def ingest_url(pdf_path: str) -> str:
    """URL content -> Markdown"""
    converter = DocumentConverter()
    result = converter.convert(pdf_path)

    return result.document.export_to_markdown()

================
File: background/sources/shared.py
================
from typing import AsyncIterator
from db.models import Source
from models.document_details import DocumentDetails


class DocumentStream:
    """Base class for streaming documents from various sources.

    Provides an async iterator interface for retrieving DocumentDetails
    from a specific source.
    """

    def __init__(self, source: Source):
        if not source:
            raise ValueError("source must be provided")
        self.source = source

    async def __aiter__(self) -> AsyncIterator[DocumentDetails]:
        """
        Asynchronous iterator method to be overridden in subclasses.

        Returns:
            AsyncIterator[DocumentDetails]: An asynchronous iterator of `DocumentDetails`.
        """
        yield DocumentDetails()  # Placeholder for subclass implementation

================
File: background/sources/ucop.py
================
from datetime import datetime
from urllib.parse import urljoin
import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup, Tag

from typing import AsyncIterator

from background.logger import setup_logger
from background.sources.ingestion import ingest_url
from background.sources.shared import DocumentStream
from db.models import Source
from models.document_details import DocumentDetails

logger = setup_logger()

# TODO: move to shared
user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# UCOP Policies are on `https://policy.ucop.edu`
base_url = "https://policy.ucop.edu"


class UcopDocumentStream(DocumentStream):
    def __init__(self, source: Source):
        super().__init__(source)
        self.policy_url = source.url
        self.max_retries = 3

    async def __aiter__(self) -> AsyncIterator[DocumentDetails]:
        # We'll use playwright to read the provided policies page and get back the list of policies
        # then we'll loop through each and grab the policy PDF, parse it and return the DocumentDetails

        async with async_playwright() as playwright:
            # Launch browser with custom user agent
            browser = await playwright.chromium.launch()
            context = await browser.new_context(user_agent=user_agent)

            # Create new page and navigate to UCOP policies
            page = await context.new_page()

            try:
                # Navigate and wait for content to load
                await page.goto(self.policy_url)
                await page.wait_for_selector("#accordion", state="visible")

                # Get page content after JavaScript execution
                content = await page.content()

                # Parse with BeautifulSoup
                soup = BeautifulSoup(content, "html.parser")
            except Exception as e:
                print(f"Couldn't fetch policy info: {str(e)}")
            finally:
                await context.close()
                await browser.close()

            # we now have the page content, let's get all policies
            # Find the element with the id 'accordion'
            accordion = soup.find(id="accordion")

            if not isinstance(accordion, Tag):
                logger.error("Couldn't find the accordion element")
                return

            # Find all 'a' tags within the accordion with class="blue"
            raw_links = accordion.find_all("a", class_="blue")
            links = [link for link in raw_links if isinstance(link, Tag)]

            for link in links:
                # Get href directly from the link tag but convert to absolute url
                raw_href = link.get("href")
                if raw_href is None:
                    logger.error(
                        "Couldn't find the href attribute in this link, moving on")
                    continue
                elif isinstance(raw_href, list):
                    # Join the list elements into a single string if necessary.
                    href_attr = " ".join(raw_href)
                else:
                    href_attr = raw_href

                href = urljoin(base_url, href_attr)

                logger.info(f"Processing policy at {href}")

                # For the title, find the first (or only) 'span' with class 'icon pdf' within the link
                span = link.find("span", class_="icon pdf")
                if span:  # Check if the span exists
                    title = span.text.strip()
                else:
                    title = "Title not found"

                # get the parent of the link and find the next 4 sibling divs - subject areas, effective date, issuance date, responsible office
                parent = link.parent

                if not isinstance(parent, Tag):
                    logger.error("Couldn't find the parent div of this link")
                    continue

                # Get the next 4 sibling divs
                raw_siblings = parent.find_next_siblings("div")
                siblings = [
                    sibling for sibling in raw_siblings if isinstance(sibling, Tag)]

                # Get the text from each sibling but ignore the <cite> tag
                subject_areas_text = get_sibling_text(siblings[0])
                effective_date = get_sibling_text(siblings[1])
                issuance_date = get_sibling_text(siblings[2])
                responsible_office = get_sibling_text(siblings[3])
                classifications = ["Policy"]

                # subject areas is a comma separated list, so split it into a list
                subject_areas = [area.strip()
                                 for area in subject_areas_text.split(",")]

                # extract the content of the policy PDF
                markdown_content = ingest_url(href)

                # Create a DocumentDetails object with the extracted information
                doc = DocumentDetails(
                    title=title,
                    url=href,
                    description="",
                    content=markdown_content,
                    last_modified=datetime.now().isoformat(),
                    metadata={
                        "subject_areas": subject_areas,
                        "effective_date": effective_date,
                        "issuance_date": issuance_date,
                        "responsible_office": responsible_office,
                        "classifications": classifications
                    }
                )

                logger.info(f"Processed policy: {doc.title} at {doc.url}")

                yield doc


def get_sibling_text(sibling: Tag) -> str:
    """
    Return the text of the sibling with any text from a <cite> tag removed.
    """
    cite_tag = sibling.find("cite")
    cite_text = cite_tag.text if cite_tag is not None else ""
    return sibling.text.replace(cite_text, "").strip()


if __name__ == "__main__":
    source = Source(
        name="UCOP",
        url="https://policy.ucop.edu/advanced-search.php?action=welcome&op=browse",
        last_updated=None,
        type="UCOP")

    async def main():
        async for doc in UcopDocumentStream(source):
            print(doc)
            print(doc.metadata)

    asyncio.run(main())

================
File: background/logger.py
================
"""
Logging and monitoring utilities for the background process.
"""

import logging
import os
import resource
import sentry_sdk

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Sentry will integrate with logger by default, but only warnings and above. this is a good default
sentry_sdk.init(
    dsn=os.getenv('SENTRY_DSN'),  # Load DSN from environment variable
    traces_sample_rate=1.0,
    profiles_sample_rate=1.0,
    # Optionally set the environment
    environment=os.getenv('ENVIRONMENT', 'development')
)


def get_log_level_from_str(log_level_str: str = "INFO") -> int:
    """Translate strings to log level."""
    log_level_dict = {
        "CRITICAL": logging.CRITICAL,
        "ERROR": logging.ERROR,
        "WARNING": logging.WARNING,
        "INFO": logging.INFO,
        "DEBUG": logging.DEBUG,
        "NOTSET": logging.NOTSET,
    }

    return log_level_dict.get(log_level_str.upper(), logging.INFO)


def setup_logger(
    name: str = __name__,
    log_level: int = get_log_level_from_str(),
    logfile_name: str | None = None,
) -> logging.LoggerAdapter:
    """Setup a logger with the given name and log level."""

    logger = logging.getLogger(name)

    # If the logger already has handlers, assume it was already configured and return it.
    if logger.handlers:
        return logger

    logger.setLevel(log_level)

    formatter = logging.Formatter(
        "%(asctime)s %(filename)20s%(lineno)4s : %(message)s",
        datefmt="%m/%d/%Y %I:%M:%S %p",
    )

    handler = logging.StreamHandler()
    handler.setLevel(log_level)
    handler.setFormatter(formatter)

    logger.addHandler(handler)

    if logfile_name:
        is_containerized = os.path.exists("/.dockerenv")
        file_name_template = (
            "/var/log/{name}.log" if is_containerized else "./log/{name}.log"
        )
        file_handler = logging.FileHandler(
            file_name_template.format(name=logfile_name))
        logger.addHandler(file_handler)

    return logger


def log_memory_usage(logger: logging.Logger):
    """Log the memory usage of the current process."""
    memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    logger.info(f"Memory Usage: {memory_usage} KB")

================
File: background/stream.py
================
from datetime import datetime, timezone
import hashlib
from typing import List

from langchain_openai import OpenAIEmbeddings
from langchain_core.embeddings import FakeEmbeddings
from background.logger import setup_logger
from background.sources.shared import DocumentStream
from background.sources.ucop import UcopDocumentStream
from db.constants import SourceType
from db.models import Document, DocumentChunk, DocumentContent, Source
from db.mutations import delete_chunks_and_content
from db.queries import get_document_by_url
from models.document_details import DocumentDetails
from sqlalchemy.orm import Session
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

logger = setup_logger()

PROCESSOR_BATCH_SIZE = 2  # small for testing


class DocumentIngestStream():
    @staticmethod
    def getSourceStream(source: Source) -> DocumentStream:
        if source.type == SourceType.UCOP:
            return UcopDocumentStream(source)
        # TODO: Add other source types here
        else:
            raise ValueError(f"Unsupported source type {source.type}")


class DocumentProcessor:
    def __init__(self, session: Session, stream: DocumentStream):
        self.session = session
        self.stream = stream
        self.batch_size = PROCESSOR_BATCH_SIZE
        self.current_batch = []

    async def process_stream(self):
        ingest_results = []

        try:
            async for document in self.stream:
                logger.info(
                    f"New document: {document} added to batch.  Current batch size: {len(self.current_batch)}")
                self.current_batch.append(document)

                if len(self.current_batch) >= self.batch_size:
                    ingest_result = await self.process_batch(self.current_batch)

                    ingest_results.append(ingest_result)
                    self.current_batch = []

            # Process any remaining documents
            if self.current_batch:
                ingest_result = await self.process_batch(self.current_batch)
                ingest_results.append(ingest_result)

            return IngestResult.combine_results(ingest_results)
        except Exception as e:
            logger.error(f"Error processing stream: {str(e)}")
            raise

    async def process_batch(self, batch: List[DocumentDetails]):
        # TODO: refactor into separate class/file

        start_time = datetime.now(timezone.utc)
        num_docs_indexed = 0
        num_new_docs = 0
        token_count = 0

        for document_details in batch:
            # - get the doc from the db and check if it has changed
            # - include hash, content_length (of entire non-chunked content) and other metadata
            # - chunk and vectorize the content, store in vector db
            # - store entire document content in a separate db table (for ctx retrieval)
            # - update/save doc in db
            logger.info(f"Processing document {document_details.url}")

            if not document_details:
                logger.warning(
                    f"Document {document_details.url} is empty. Skipping")
                continue

            num_docs_indexed += 1

            content_hash = calculate_content_hash(document_details.content)

            # add the content hash so we have it for later
            document_details.metadata['hash'] = content_hash

            # get the doc from the db
            db_document = get_document_by_url(
                self.session, document_details.url)

            if db_document and db_document.meta and db_document.meta.get("hash") == content_hash:
                logger.info(
                    f"Document {document_details.url} has not changed. Skipping")
                continue

            # doc changed, let's vectorize and update
            num_new_docs += 1
            db_document = self.vectorize_document(
                document_details, db_document)

            # TODO: do we want to commit here? or just keep flushing? Seems to me we might want to comit after each doc?
            self.session.commit()

        # end of batch
        logger.info(
            f"Indexed {num_docs_indexed} documents from source {self.stream.source.name} with {token_count} tokens")

        end_time = datetime.now(timezone.utc)

        return IngestResult(
            num_docs_indexed=num_docs_indexed,
            num_new_docs=num_new_docs,
            source_id=self.stream.source.id,
            start_time=start_time,
            end_time=end_time,
            duration=(end_time - start_time).total_seconds()
        )

    def vectorize_document(self, document_details: DocumentDetails, db_document: Document | None):
        # we want to re-vectorize the document if it has changed, or otherwise create new
        # 1. chunk the content
        # 2. vectorize the chunks
        # 3. remove any existing content or chunks related to the doc
        # 4. store the new content and vectors
        # 5. return the updated db document

        # 1. We're going to use langchain to chunk using the content (assuming it's markdown)
        # https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/
        headers_to_split_on = [
            ("#", "h1"),
            ("##", "h2"),
            ("###", "h3"),
        ]

        # first we split the content into headers and content
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on, strip_headers=False)
        md_header_splits = markdown_splitter.split_text(
            document_details.content)

        # now we need to chunk the content into smaller pieces so it can be vectorized
        # langchain uses character sizes so token sizes are about 4x the character size
        chunk_size_in_tokens = 500
        chunk_overlap_in_tokens = 50
        chunk_size = chunk_size_in_tokens * 4
        chunk_overlap = chunk_overlap_in_tokens * 4
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )

        chunks = text_splitter.split_documents(md_header_splits)

        # log the chunks
        logger.info(
            f"Document {document_details.url} has {len(chunks)} chunks")

        # 2. vectorize the chunks
        # embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
        embeddings = FakeEmbeddings(size=1536)

        # mass embeddings for all chunks
        embedded_vectors = embeddings.embed_documents(
            [chunk.page_content for chunk in chunks])

        # 3. remove any existing content or chunks related to the doc
        if db_document:
            # remove existing content and chunks
            delete_chunks_and_content(self.session, db_document)

        # 4. store the new content and vectors
        doc_chunks = [
            DocumentChunk(
                chunk_index=i,
                chunk_text=chunk.page_content,
                embedding=embedded_vectors[i],
                meta={
                    **(chunk.metadata if hasattr(chunk, 'metadata') and chunk.metadata else {}),
                    "parent_document_length": len(document_details.content)
                }
            )
            for i, chunk in enumerate(chunks)]

        doc_content = DocumentContent(content=document_details.content)

        # get the metadata for this doc
        metadata = document_details.metadata or {}

        # 5. update the db document info
        if not db_document:
            db_document = Document()

        db_document.title = document_details.title
        db_document.source_id = self.stream.source.id
        db_document.last_updated = datetime.now(timezone.utc)
        db_document.url = document_details.url
        db_document.meta = metadata

        # # save the doc
        self.session.add(db_document)
        # self.session.flush()

        # now that we've flushed the changes and have a docId we can continue
        db_document.chunks = doc_chunks
        db_document.content = doc_content

        return db_document


class IngestResult:

    def __init__(
        self,
        num_docs_indexed,
        num_new_docs,
        source_id,
        start_time,
        end_time,
        duration,
    ):
        self.num_docs_indexed = num_docs_indexed
        self.num_new_docs = num_new_docs
        self.source_id = source_id
        self.start_time = start_time
        self.end_time = end_time
        self.duration = duration

    @staticmethod
    def combine_results(results: List['IngestResult']) -> 'IngestResult':
        """
        Combines multiple ingest results into a single result by summing their values.
        SourceId must be the same across all results.
        Time values reflect the total span across all results.

        Args:
            results (List[IngestResult]): List of IngestResult objects to combine

        Returns:
            IngestResult: Combined result with summed values
        """
        if not results:
            return get_bad_ingest_result()

        # make sure all source ids are the same
        if len(set(r.source_id for r in results)) > 1:
            raise ValueError(
                "All source ids must be the same to combine results")

        total_docs_indexed = sum(r.num_docs_indexed for r in results)
        total_new_docs = sum(r.num_new_docs for r in results)
        total_duration = sum(r.duration for r in results)
        start_time = min(r.start_time for r in results)
        end_time = max(r.end_time for r in results)

        # Keep the source_id from the first result as specified
        source_id = results[0].source_id

        return IngestResult(
            num_docs_indexed=total_docs_indexed,
            num_new_docs=total_new_docs,
            source_id=source_id,
            start_time=start_time,
            end_time=end_time,
            duration=total_duration
        )


def get_bad_ingest_result() -> IngestResult:
    return IngestResult(
        num_docs_indexed=0,
        num_new_docs=0,
        source_id=None,
        start_time=datetime.now(timezone.utc),
        end_time=datetime.now(timezone.utc),
        duration=0,
    )


def calculate_content_hash(content: str) -> str:
    hasher = hashlib.sha256()
    hasher.update(content.encode())
    return hasher.hexdigest()


# if __name__ == "__main__":
#     # load sample policy from file
#     with open("/workspace/backend/experiments/data/sample_policy.md", "r") as f:
#         content = f.read()

#     fake_document_details = DocumentDetails(
#         content=content)

#     vectorize_document(fake_document_details, None)

================
File: background/update.py
================
"""
Main module to update data for RAG
"""
import asyncio
from datetime import datetime, timedelta, timezone
import gc
import math
import traceback
from dotenv import load_dotenv
import time
from sqlalchemy.orm import Session

from background.logger import setup_logger
from background.stream import DocumentIngestStream, DocumentProcessor
from db.mutations import create_index_attempt
from db.models import IndexAttempt, Source
from db.constants import IndexStatus, RefreshFrequency, SourceStatus
from db.connection import get_session
from db.queries import get_sources

logger = setup_logger()

load_dotenv()  # This loads the environment variables from .env

# TODO: load from env
MAX_SOURCE_FAILURES = 3


async def index_documents(session: Session, source: Source) -> None:
    start_time = datetime.now(timezone.utc)

    logger.info(f"Indexing source {source.name}")

    # create new index attempt
    attempt = create_index_attempt(session, source, start_time)

    # each source needs to implement a DocumentStream class that returns an async iterator of DocumentDetail objects
    # we'll use that along with our IngestProcessor to handle the actual ingestion
    # our ingest and processor are async, so we can handle sources which don't get their data all at once (like APIs or crawling)
    # our ingest will check if the document already exists in the db, if so, update the metadata and text, if not, create a new one.
    # use hash and last_modified to check if file has changed
    # then when all are done, update the source last_updated field and update the attempt with the final counts
    # OPTIONAL: eventually, we could add a check to see if the doc has been removed from the source, and if so, remove it from the db

    try:
        # our docs will be streamed in, since we might not get the list all at once
        stream = DocumentIngestStream.getSourceStream(source)
        processor = DocumentProcessor(session, stream)

        # Process documents as they arrive
        processor_result = await processor.process_stream()

        logger.info(f"Indexing source {source.name} successful.")

        # End timing the indexing attempt
        end_time = datetime.now(timezone.utc)

        # Record a successful index attempt
        attempt.status = IndexStatus.SUCCESS
        attempt.end_time = end_time
        attempt.duration = math.floor((end_time - start_time).total_seconds())
        attempt.num_docs_indexed = processor_result.num_docs_indexed
        attempt.num_new_docs = processor_result.num_new_docs
        attempt.num_docs_removed = 0  # TODO: update with actual counts

        source.last_updated = datetime.now(timezone.utc)
        source.failure_count = 0
        source.last_failed = None

        session.commit()

    except Exception as e:
        end_time = datetime.now(timezone.utc)

        # Record a failed index attempt
        attempt.status = IndexStatus.FAILURE
        attempt.error_details = traceback.format_exc()  # Get full traceback
        attempt.end_time = end_time

        logger.warning(f"Indexing failed for source: {source.name} due to {e}")

        # register failed attempts.  If too many failed attempts, disable the source
        source.last_failed = datetime.now(timezone.utc)
        source.failure_count += 1

        if source.failure_count >= MAX_SOURCE_FAILURES:
            logger.error(
                f"Source {source.name} has failed {source.failure_count} times. Disabling."
            )
            source.status = SourceStatus.FAILED

        session.commit()


async def update_loop(delay: int = 60) -> None:
    """Function to update data."""
    while True:
        start = time.time()
        start_time_utc = datetime.fromtimestamp(
            start).strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"Running update, current UTC time: {start_time_utc}")

        # get all sources that might need to be updated (daily and last updated more than 1 day ago)
        one_day_ago = datetime.now() - timedelta(days=1)

        with get_session() as session:
            # new session for each loop iteration
            sources_to_index = get_sources(session, one_day_ago,
                                           refresh_frequency=RefreshFrequency.DAILY)

            # we don't want to index any sources that have failed recently
            filtered_sources = []
            current_time = datetime.now()

            for source in sources_to_index:
                if (
                    not source.last_failed
                ):  # if the source has never failed, add it to the list
                    filtered_sources.append(source)
                else:  # if the source has failed, check if it has been long enough to try again
                    allowable_failure_time = source.last_failed + timedelta(
                        hours=source.failure_count * 6
                    )
                    if allowable_failure_time <= current_time:
                        filtered_sources.append(source)

            if not filtered_sources:
                logger.info(
                    "No sources to update. Checking for collections to sync")

                # TODO: will need to rewrite collection sync to work with pgvector/pgsql
                # only once all sources are up to date - attempt to sync collections if needed (only one at a time)
                # sync_first_collection_if_needed(session)

                # if we have on inprogress attempts, they are probably stale, so clean them up since there is nothing else to do
                cleanup_old_attempts(session)

                time.sleep(delay)
                continue

            # we have valid sources in need up update, get the first
            source = filtered_sources[0]

            # Perform indexing
            await index_documents(session, source)

        gc.collect()  # clean up memory after each indexing run

        # end of loop, go back and check for more sources to index


def cleanup_old_attempts(session: Session) -> None:
    """Set to failed any index_attempts that are INPROGRESS and started more than 1 day ago"""
    one_day_ago = datetime.now(timezone.utc) - timedelta(days=1)

    session.query(IndexAttempt).filter(
        IndexAttempt.status == IndexStatus.INPROGRESS,
        IndexAttempt.start_time <= one_day_ago,
    ).update(
        {
            IndexAttempt.status: IndexStatus.FAILURE,
            IndexAttempt.error_details: "Indexing attempt took too long or was interrupted",
        }
    )


async def update__main() -> None:
    """Main function to run the update loop."""
    logger.info("Starting Indexing Loop")
    await update_loop()


if __name__ == "__main__":
    asyncio.run(update__main())

================
File: db/connection.py
================
import os
from dotenv import load_dotenv
from sqlalchemy import Engine, create_engine
from sqlalchemy.orm import Session

load_dotenv()  # This loads the environment variables from .env

# Dictionary to store engines for different database URLs
_db_engines: dict[str, Engine] = {}


def get_default_db_url() -> str:
    # Ensure the connection string is fetched from environment variable
    DATABASE_URL = os.getenv('DATABASE_URL')
    if not DATABASE_URL:
        raise ValueError("DATABASE_URL environment variable not set")
    return DATABASE_URL


def get_db_engine(db_url: str = None) -> Engine:
    if db_url is None:
        db_url = get_default_db_url()

    if db_url not in _db_engines:
        _db_engines[db_url] = create_engine(db_url)

    return _db_engines[db_url]


def get_session(db_url: str = None) -> Session:
    return Session(bind=get_db_engine(db_url))

================
File: db/constants.py
================
"""
Parameters to control indexing and source status
"""
from enum import Enum


class RefreshFrequency(Enum):
    """Enum for the refresh frequency of a source."""
    DAILY = "DAILY"
    WEEKLY = "WEEKLY"


class IndexStatus(Enum):
    """Enum for the status of an index."""
    SUCCESS = "SUCCESS"
    FAILURE = "FAILURE"
    INPROGRESS = "INPROGRESS"


class SourceStatus(Enum):
    """Enum for the status of a source."""
    ACTIVE = "ACTIVE"
    INACTIVE = "INACTIVE"
    FAILED = "FAILED"
    DEACTIVATE = "DEACTIVATE"


class SourceType(Enum):
    """Enum for the type of a source."""
    UCOP = "UCOP"  # Crawl UCOP policies
    UCDPOLICYMANUAL = "UCDPOLICYMANUAL"  # Crawl UCD Policy Manual (ellucid)
    UCCONTRACTS = "UCCONTRACTS"  # Crawl UC Contracts and Bargaining Agreements
    # (unsupported) Given a base site, index everything under that path
    RECURSIVE = "RECURSIVE"
    # (unsupported) Given a sitemap.xml URL, parse all the pages in it
    SITEMAP = "SITEMAP"


class RoleName(Enum):
    """Enum for the role of a user."""
    ADMIN = "ADMIN"
    USER = "USER"

================
File: db/models.py
================
"""
Data models for sources, indices, and documents
"""

from datetime import datetime
from typing import Optional, List
from sqlalchemy import JSON, Boolean, Column, DateTime, Enum, Float, ForeignKey, Index, Integer, Table, Text, func
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy.orm import Mapped
from sqlalchemy.orm import mapped_column
from sqlalchemy.orm import relationship
from sqlalchemy import String

from db.constants import IndexStatus, RefreshFrequency, RoleName, SourceStatus, SourceType
from pgvector.sqlalchemy import Vector


class Base(DeclarativeBase):
    pass


class Chat(Base):
    # Chat is a single chat instance, which can contain multiple messages
    __tablename__ = "chats"

    # id is provided by the chat platform (nextJS)
    id: Mapped[str] = mapped_column(String, primary_key=True)

    title: Mapped[str] = mapped_column(String, nullable=False)

    # full list of messages
    messages: Mapped[dict] = mapped_column(JSON, nullable=False)

    # which chat assistant is being used
    assistant_slug: Mapped[str] = mapped_column(
        ForeignKey("assistants.slug"), nullable=False)

    # llm model being used
    llm_model: Mapped[str] = mapped_column(String, nullable=False)

    user_id: Mapped[int] = mapped_column(
        ForeignKey("users.id"), nullable=False)
    user: Mapped[Optional["User"]] = relationship(
        "User", back_populates="chats")

    timestamp: Mapped[datetime] = mapped_column(
        DateTime, default=func.now(), nullable=False)

    share_id: Mapped[Optional[str]] = mapped_column(String, nullable=True)

    active: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)

    # store focus and other meta data
    meta: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)

    assistant = relationship("Assistant", back_populates="chats")

    # Define indexes
    __table_args__ = (
        Index('idx_userid_active_timestamp', 'user_id', 'active', 'timestamp'),
        Index('idx_id_active', 'id', 'active')
    )


class Assistant(Base):
    __tablename__ = "assistants"

    # Primary key (auto-incremented ID)
    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)

    # Slug used in the URL (unique)
    slug: Mapped[str] = mapped_column(String, unique=True, nullable=False)

    # Assistant name and description
    name: Mapped[str] = mapped_column(String, nullable=False)

    description: Mapped[str] = mapped_column(String, nullable=False)

    # Theme as a choice between gunrock and sage (Enum in SQLAlchemy)
    theme: Mapped[str] = mapped_column(String, nullable=False)

    # Relationship with chats (one-to-many)
    chats = relationship("Chat", back_populates="assistant")

    instructions: Mapped[str] = mapped_column(String, nullable=True)

    # Relationship with default questions (one-to-many)
    default_questions = relationship(
        "DefaultQuestion", back_populates="assistant")

    collections: Mapped[List["Collection"]] = relationship(
        "Collection",
        secondary="assistant_collections",
        back_populates="assistants"
    )


class AssistantCollection(Base):
    """
    Join table for assistants and collections
    """
    __tablename__ = "assistant_collections"

    assistant_id: Mapped[int] = mapped_column(
        ForeignKey("assistants.id"), primary_key=True)
    collection_id: Mapped[int] = mapped_column(
        ForeignKey("collections.id"), primary_key=True)

    __table_args__ = (
        Index('ix_assistant_collections_assistant_id', 'assistant_id'),
        Index('ix_assistant_collections_collection_id', 'collection_id'),
    )


class DefaultQuestion(Base):
    __tablename__ = "default_questions"

    # Primary key (auto-incremented ID)
    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)

    # Foreign key to link to the assistant's slug
    assistant_slug: Mapped[str] = mapped_column(
        ForeignKey("assistants.slug"), nullable=False)

    # Default question text
    question: Mapped[str] = mapped_column(String, nullable=False)

    # Relationship back to the Assistant model
    assistant = relationship(
        "Assistant", back_populates="default_questions")


class Source(Base):
    # Source is a source of documents, usually a website
    __tablename__ = "sources"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    name: Mapped[str] = mapped_column(String, nullable=False)

    # Web URL of the source for websites
    url: Mapped[str] = mapped_column(String, nullable=False)

    refresh_frequency: Mapped[RefreshFrequency] = mapped_column(
        Enum(RefreshFrequency, native_enum=False), nullable=False
    )

    status: Mapped[SourceStatus] = mapped_column(
        Enum(SourceStatus, native_enum=False), nullable=False)

    type: Mapped[SourceType] = mapped_column(
        Enum(SourceType, native_enum=False), nullable=False)

    # allow dict of custom config values
    # TODO: use pydantic model: https://gist.github.com/srkirkland/953551872c5cb5838bde035413a8da32
    config: Mapped[dict] = mapped_column(JSON, nullable=True)

    last_updated: Mapped[Optional[datetime]] = mapped_column(
        DateTime, nullable=True
    )

    last_failed: Mapped[Optional[datetime]
                        ] = mapped_column(DateTime, nullable=True)

    failure_count: Mapped[int] = mapped_column(
        Integer, default=0, nullable=False)

    index_attempts: Mapped[list['IndexAttempt']] = relationship(
        "IndexAttempt", back_populates="source")

    documents: Mapped[list['Document']] = relationship(
        "Document", back_populates="source")

    collections: Mapped[list['Collection']] = relationship(
        "Collection",
        secondary="collections_sources",
        back_populates="sources")

    __table_args__ = (
        Index('ix_source_refresh_status_last_updated',
              'refresh_frequency',
              'status',
              'last_updated'),
    )


class IndexAttempt(Base):
    __tablename__ = "index_attempts"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    status: Mapped[IndexStatus] = mapped_column(
        Enum(IndexStatus, native_enum=False), nullable=False)

    num_docs_indexed: Mapped[int] = mapped_column(Integer, default=0)
    num_new_docs: Mapped[int] = mapped_column(Integer, default=0)
    num_docs_removed: Mapped[int] = mapped_column(Integer, default=0)

    start_time: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    end_time: Mapped[Optional[datetime]] = mapped_column(
        DateTime, nullable=True)

    duration: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    error_details: Mapped[Optional[str]] = mapped_column(String, nullable=True)

    # Foreign key to sources
    source_id: Mapped[int] = mapped_column(
        ForeignKey("sources.id"), nullable=False)

    # Relationship to source
    source: Mapped["Source"] = relationship(
        "Source", back_populates="index_attempts")


class Document(Base):
    __tablename__ = "documents"

    # id is an auto-incremented integer primary key
    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)

    # title is the title of the document
    title: Mapped[str] = mapped_column(String, nullable=False)

    # full URL
    url: Mapped[Optional[str]] = mapped_column(
        String, nullable=True, index=True)

    meta: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)

    source_id: Mapped[int] = mapped_column(
        ForeignKey("sources.id"), nullable=False)
    source: Mapped["Source"] = relationship(
        "Source", back_populates="documents")

    last_updated: Mapped[Optional[datetime]] = mapped_column(
        DateTime, nullable=True
    )

    chunks: Mapped[List["DocumentChunk"]] = relationship(
        "DocumentChunk", back_populates="document")

    content: Mapped["DocumentContent"] = relationship(
        "DocumentContent", back_populates="document", uselist=False, cascade="all, delete-orphan")


class DocumentChunk(Base):
    __tablename__ = "document_chunks"
    __table_args__ = (
        # Index for similarity search on the vector column
        Index(
            "document_chunks_embedding_idx",
            "embedding",
            postgresql_using="hnsw",
            postgresql_ops={"embedding": "vector_cosine_ops"}
        ),
    )

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    # Reference to the main document (documents.id is a string)
    document_id: Mapped[int] = mapped_column(
        Integer, ForeignKey("documents.id"), nullable=False)
    # The order/index of the chunk
    chunk_index: Mapped[int] = mapped_column(Integer, nullable=False)
    # The actual text of this chunk
    chunk_text: Mapped[str] = mapped_column(Text, nullable=False)
    # The vector embedding for this chunk (adjust dimension as needed)
    embedding: Mapped[List[float]] = mapped_column(
        Vector(1536), nullable=False)
    # Optional metadata stored as JSONB
    meta: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)

    # Back-reference to the Document
    document: Mapped["Document"] = relationship(
        "Document", back_populates="chunks")


class DocumentContent(Base):
    __tablename__ = "document_contents"

    # Use the document_id as the primary key and foreign key to documents.id
    document_id: Mapped[int] = mapped_column(
        Integer, ForeignKey("documents.id", ondelete='CASCADE'), primary_key=True)
    # The full, potentially very large, document content
    content: Mapped[str] = mapped_column(Text, nullable=False)

    # Back-reference to the Document
    document: Mapped["Document"] = relationship(
        "Document", back_populates="content")


user_roles = Table(
    "user_roles",
    Base.metadata,
    Column("user_id", ForeignKey("users.id"), primary_key=True),
    Column("role_id", ForeignKey("roles.id"), primary_key=True),
)


class User(Base):
    # store users that will have roles in the system
    __tablename__ = "users"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    name: Mapped[str] = mapped_column(String, nullable=False)
    email: Mapped[str] = mapped_column(String, nullable=False)
    kerberos: Mapped[str] = mapped_column(String(20), nullable=True)
    # TODO: make iam unique/notnull once we have all the data
    iam: Mapped[Optional[str]] = mapped_column(String(10), nullable=True)
    ms_user_id: Mapped[str] = mapped_column(
        String, nullable=False, unique=True)
    titles: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    affiliations: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    departments: Mapped[Optional[str]
                        ] = mapped_column(String, nullable=True)

    roles: Mapped[List["Role"]] = relationship(
        secondary=user_roles, back_populates="users")
    chats: Mapped[List["Chat"]] = relationship("Chat", back_populates="user")

    __table_args__ = (
        Index('ix_users_name', 'name'),
        Index('ix_users_email', 'email'),
        Index('ix_users_kerberos', 'kerberos'),
    )


class Role(Base):
    # Role is a model for roles
    __tablename__ = "roles"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)

    name: Mapped[RoleName] = mapped_column(
        Enum(RoleName, native_enum=False), nullable=False)

    users: Mapped[List["User"]] = relationship(
        secondary=user_roles, back_populates="roles")


class Prompt(Base):
    __tablename__ = "prompts"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    prompt: Mapped[str] = mapped_column(String, nullable=False)
    expected_output: Mapped[Optional[str]
                            ] = mapped_column(String, nullable=True)
    category: Mapped[str] = mapped_column(String, nullable=False)
    comments: Mapped[Optional[str]] = mapped_column(String, nullable=True)

    prompt_evaluations: Mapped[List["PromptEvaluation"]] = relationship(
        "PromptEvaluation", back_populates="prompt")


class Evaluation(Base):
    __tablename__ = "evaluations"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    assistant_id: Mapped[int] = mapped_column(
        ForeignKey("assistants.id"), nullable=False)
    pipeline_version: Mapped[str] = mapped_column(String, nullable=False)
    comments: Mapped[Optional[str]] = mapped_column(String, nullable=True)
    run_date: Mapped[DateTime] = mapped_column(DateTime, nullable=False)
    overall_score: Mapped[float] = mapped_column(Float, nullable=False)

    prompt_evaluations: Mapped[List["PromptEvaluation"]] = relationship(
        "PromptEvaluation", back_populates="evaluation")


class PromptEvaluation(Base):
    __tablename__ = "prompt_evaluations"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    prompt_id: Mapped[int] = mapped_column(
        ForeignKey("prompts.id"), nullable=False)
    evaluation_id: Mapped[int] = mapped_column(
        ForeignKey("evaluations.id"), nullable=False)
    actual_output: Mapped[str] = mapped_column(String, nullable=False)
    context: Mapped[str] = mapped_column(String, nullable=True)
    scores: Mapped[dict] = mapped_column(JSON, nullable=True)
    overall_score: Mapped[float] = mapped_column(Float, nullable=False)

    prompt: Mapped["Prompt"] = relationship(
        "Prompt", back_populates="prompt_evaluations")
    evaluation: Mapped["Evaluation"] = relationship(
        "Evaluation", back_populates="prompt_evaluations")


class Collection(Base):
    """
    Represents a collection of sources.  Used by the assistant to determine which sources to use.
    """
    __tablename__ = "collections"

    id: Mapped[int] = mapped_column(
        Integer, primary_key=True, autoincrement=True)
    name: Mapped[str] = mapped_column(String, nullable=False, unique=True)
    created_date: Mapped[datetime] = mapped_column(
        DateTime, default=func.now(), nullable=False)
    active: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)

    # true if we need to sync the collection mapping with the index
    requires_sync: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=True)

    sources: Mapped[List["Source"]] = relationship(
        "Source",
        secondary="collections_sources",
        back_populates="collections"
    )

    assistants: Mapped[List["Assistant"]] = relationship(
        "Assistant",
        secondary="assistant_collections",
        back_populates="collections"
    )


class CollectionSource(Base):
    """
    Join table for collections and sources
    """
    __tablename__ = "collections_sources"

    collection_id: Mapped[int] = mapped_column(
        ForeignKey("collections.id"), primary_key=True)
    source_id: Mapped[int] = mapped_column(
        ForeignKey("sources.id"), primary_key=True)

    __table_args__ = (
        Index('ix_collections_sources_collection_id', 'collection_id'),
        Index('ix_collections_sources_source_id', 'source_id'),
    )

================
File: db/mutations.py
================
from datetime import datetime

from sqlalchemy import delete
from sqlalchemy.orm import Session

from db.constants import IndexStatus
from db.models import Document, DocumentChunk, DocumentContent, IndexAttempt, Source


def create_index_attempt(session: Session, source: Source, start_time: datetime) -> IndexAttempt:
    # create new index attempt
    attempt = IndexAttempt(
        source_id=source.id,
        status=IndexStatus.INPROGRESS,
        num_docs_indexed=0,
        num_new_docs=0,
        num_docs_removed=0,
        start_time=start_time,
        duration=0,
        end_time=None,
        error_details=None,
    )

    session.add(attempt)
    session.commit()

    return attempt


def delete_chunks_and_content(session: Session, document: Document) -> None:
    # delete all chunks and content for the document
    session.execute(delete(DocumentChunk).where(
        DocumentChunk.document_id == document.id))

    session.execute(delete(DocumentContent).where(
        DocumentContent.document_id == document.id))

================
File: db/queries.py
================
from datetime import datetime
from sqlalchemy.orm import Session

from db.models import Document, Source
from db.constants import RefreshFrequency, SourceStatus

from sqlalchemy import or_


def get_sources(session: Session, last_updated: datetime, refresh_frequency: RefreshFrequency = RefreshFrequency.DAILY, status: SourceStatus = SourceStatus.ACTIVE) -> list['Source']:
    return session.query(Source).filter(
        or_(Source.last_updated < last_updated, Source.last_updated.is_(None)),
        Source.refresh_frequency == refresh_frequency,
        Source.status == status
    ).all()


def get_document_by_url(session: Session, url: str) -> Document | None:
    return session.query(Document).filter(Document.url == url).first()

================
File: dev/reset_db.py
================
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

from alembic import command
from alembic.config import Config

load_dotenv()  # This loads the environment variables from .env

# Ensure the connection string is fetched from environment variable
DATABASE_URL = os.getenv('DATABASE_URL')
if not DATABASE_URL:
    raise ValueError("DATABASE_URL environment variable not set")

# only run on local pg with '@postgres:5432/' in the DATABASE_URL
if '@postgres:5432/' not in DATABASE_URL:
    raise ValueError(
        "This script is only intended to run on the local PostgreSQL database.")

# Create an engine
engine = create_engine(DATABASE_URL)

# Create a configured "Session" class
Session = sessionmaker(bind=engine)

# Create a Session
session = Session()


def reset_database():
    """
    Resets the database to the initial state by performing the following steps:
    1. Downgrades the database to the base version.
    2. Upgrades the database to the latest version.

    This function uses Alembic for database migrations and assumes that the
    Alembic configuration file is named 'alembic.ini'.

    The migration scripts themselves contain INSERT statements to populate a
    minimal set of data (roles, default questions, an assistant, etc).
    """
    alembic_cfg = Config("alembic.ini")
    command.downgrade(alembic_cfg, "base")
    command.upgrade(alembic_cfg, "head")

    print("Database has been reset to the test state.")


if __name__ == "__main__":
    reset_database()
    session.close()

================
File: dev/reset_to_dev_state.py
================
# reset the db to the dev state with

import os
from dotenv import load_dotenv
from db.models import Source
from dev.reset_db import reset_database

from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

load_dotenv()  # This loads the environment variables from .env

# Ensure the connection string is fetched from environment variable
DATABASE_URL = os.getenv('DATABASE_URL')
if not DATABASE_URL:
    raise ValueError("DATABASE_URL environment variable not set")

# only run on local pg with '@postgres:5432/' in the DATABASE_URL
if '@postgres:5432/' not in DATABASE_URL:
    raise ValueError(
        "This script is only intended to run on the local PostgreSQL database.")

# Create an engine
engine = create_engine(DATABASE_URL)

# Create a configured "Session" class
Session = sessionmaker(bind=engine)

# Create a Session
session = Session()


def reset_to_dev_state():
    # first full reset
    reset_database()

    # now insert a sample source
    # UCOP but just the fist page
    source = Source(
        name="UCOP Policies Page 1",
        url="https://policy.ucop.edu/advanced-search.php?action=welcome&op=browse",
        type="UCOP",
        status="ACTIVE",
        refresh_frequency="DAILY",
    )

    session.add(source)

    session.commit()
    print("Database has been reset to the dev state.")
    session.close()


if __name__ == "__main__":
    reset_to_dev_state()

================
File: models/document_details.py
================
import json
import re
from typing import Any, Dict, List, Optional


class DocumentDetails:
    """
    Represents the details of a document. Will be used as common metadata for all docs
    """

    def __init__(
        self,
        title="",
        url="",
        description="",
        content="",
        last_modified="",
        metadata: Optional[Dict[str, Any]] = None,
    ):
        self.title = title
        self.filename = sanitize_filename(title)
        self.url = url
        # description is a short summary of the document
        self.description = description
        # content is the full text of the document. do not include in metadata
        self.content = content
        self.last_modified = last_modified
        self.metadata = metadata or {}

    def to_vectorized_document(self, text: str):
        return VectorDocument(
            text,
            {
                "title": self.title,
                "filename": self.filename,
                "url": self.url,
                "description": self.description,
                "last_modified": self.last_modified
            }
        )

    def __str__(self):
        return f"{self.title} - {self.url} - {self.description} - {self.last_modified}"


def sanitize_filename(filename):
    """Sanitize the filename by removing or replacing invalid characters."""
    return re.sub(r'[\\/*?:"<>|]', "", filename)


class VectorDocument:
    def __init__(self, text: str, metadata: dict):
        self.text: str = text
        self.metadata: dict = metadata

    def __str__(self):
        metadata_str = json.dumps(self.metadata, indent=4)
        return f"Text: {self.text}\nMetadata: {metadata_str}"

    def update_metadata(self, new_metadata: Dict[str, Any]):
        """
        Instance method to update the metadata dictionary of this VectorDocument.

        :param new_metadata: Dictionary containing metadata entries to update or add
        """
        self.metadata.update(new_metadata)

================
File: alembic.ini
================
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = %(DATABASE_URL)


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

================
File: env.py
================
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = None

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

================
File: README.md
================
## Setting Up the Database

1. In the terminal, navigate to the `backend` directory.

2. Run `alembic upgrade head` to apply the latest database migrations.

3. (optional) Run `python dev/reset_db.py` to seed the database with some test data (WARNING: removes any old data).

# Browsing / Scraping

Playing with using playwright for browser automation instead of selenium (mostly for ease of setup)

`playwright install chromium` and `playwright install-deps chromium` currently needs to be run before using anything that requires crawling. Eventually will be built into the devcontainer or at least deployment docker images.

```bash
playwright install chromium && playwright install-deps chromium
```

# Fun with LLMs

Pack up the backend with repomix to get a full representation of the backend codebase.

```bash
npx repomix backend --ignore "experiments/,.env"
```

================
File: requirements.txt
================
alembic==1.13.2
autopep8==2.3.1
beautifulsoup4
docling
langchain==0.3.20
langchain-openai==0.3.7
langchain-text-splitters==0.3.6
pgvector==0.3.6
psycopg2==2.9.9
pytest-playwright
PyMuPDF==1.25.2
pymupdf4llm==0.0.17
python-dotenv==1.0.1
sentry-sdk==2.14.0
SQLAlchemy==2.0.33
requests==2.32.2



================================================================
End of Codebase
================================================================
